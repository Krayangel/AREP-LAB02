{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1\n",
    "\n",
    "In this step we will first load the information using panda\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "df = pd.read_csv('Heart_Disease_Prediction.csv')\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Heart Disease\"] = df[\"Heart Disease\"].map({\"presence\": 1, \"Absence\": 0})\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prep 70/30\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['Age', 'Chest pain type', 'BP', 'Max HR', 'ST depression', 'Cholesterol', 'Number of vessels fluro']\n",
    "X = df[features]\n",
    "Y = df['Heart Disease']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Target Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot distribution of the target variable\n",
    "plt.figure(figsize=(6, 4))\n",
    "df['Heart Disease'].value_counts().plot(kind='bar', color=['skyblue', 'salmon'])\n",
    "plt.title('Heart Disease Class Distribution')\n",
    "plt.xlabel('Class (0=Absence, 1=Presence)')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting and Normalization (70/30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomize and Split (70/30)\n",
    "np.random.seed(42)\n",
    "indices = np.random.permutation(len(X))\n",
    "test_size = int(len(X) * 0.3)\n",
    "test_indices = indices[:test_size]\n",
    "train_indices = indices[test_size:]\n",
    "\n",
    "X_train = X.iloc[train_indices]\n",
    "y_train = Y.iloc[train_indices]\n",
    "X_test = X.iloc[test_indices]\n",
    "y_test = Y.iloc[test_indices]\n",
    "\n",
    "# Normalize (Z-score) using parameters from training set\n",
    "mean = X_train.mean()\n",
    "std = X_train.std()\n",
    "\n",
    "X_train = (X_train - mean) / std\n",
    "X_test = (X_test - mean) / std\n",
    "\n",
    "print(\"Training set shape:\", X_train.shape)\n",
    "print(\"Test set shape:\", X_test.shape)\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Implement Basic Logistic Regression\n",
    "Sigmoid, cost (binary cross-entropy), GD (gradients, updates; track costs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def compute_cost(X, y, w, b):\n",
    "    m = len(y)\n",
    "    z = np.dot(X, w) + b\n",
    "    f_wb = sigmoid(z)\n",
    "    # Clip to avoid log(0)\n",
    "    epsilon = 1e-15\n",
    "    f_wb = np.clip(f_wb, epsilon, 1 - epsilon)\n",
    "    cost = -1/m * np.sum(y * np.log(f_wb) + (1 - y) * np.log(1 - f_wb))\n",
    "    return cost\n",
    "\n",
    "def compute_gradient(X, y, w, b):\n",
    "    m = len(y)\n",
    "    z = np.dot(X, w) + b\n",
    "    f_wb = sigmoid(z)\n",
    "    err = f_wb - y\n",
    "    dj_dw = 1/m * np.dot(X.T, err)\n",
    "    dj_db = 1/m * np.sum(err)\n",
    "    return dj_dw, dj_db\n",
    "\n",
    "def gradient_descent(X, y, w_in, b_in, alpha, num_iters):\n",
    "    w = w_in\n",
    "    b = b_in\n",
    "    J_history = []\n",
    "    \n",
    "    for i in range(num_iters):\n",
    "        dj_dw, dj_db = compute_gradient(X, y, w, b)\n",
    "        w = w - alpha * dj_dw\n",
    "        b = b - alpha * dj_db\n",
    "        \n",
    "        if i % 100 == 0 or i == num_iters-1:\n",
    "            cost = compute_cost(X, y, w, b)\n",
    "            J_history.append(cost)\n",
    "            # print(f\"Iteration {i}: Cost {cost}\")\n",
    "            \n",
    "    return w, b, J_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train on full train set\n",
    "np.random.seed(1)\n",
    "initial_w = np.zeros(X_train.shape[1])\n",
    "initial_b = 0\n",
    "iterations = 1000\n",
    "alpha = 0.01\n",
    "\n",
    "w_final, b_final, J_hist = gradient_descent(X_train.to_numpy(), y_train.to_numpy(), initial_w, initial_b, alpha, iterations)\n",
    "\n",
    "# Plot cost vs iterations\n",
    "plt.plot(range(0, iterations + 1, 100), J_hist)\n",
    "plt.title('Cost vs Iterations')\n",
    "plt.ylabel('Cost')\n",
    "plt.xlabel('Iterations (per 100)')\n",
    "plt.show()\n",
    "\n",
    "print(f\"Final Cost: {J_hist[-1]}\")\n",
    "print(f\"Final Weights: {w_final}\")\n",
    "print(f\"Final Bias: {b_final}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict and Evaluate\n",
    "def predict(X, w, b):\n",
    "    z = np.dot(X, w) + b\n",
    "    return (sigmoid(z) >= 0.5).astype(int)\n",
    "\n",
    "y_pred_train = predict(X_train.to_numpy(), w_final, b_final)\n",
    "y_pred_test = predict(X_test.to_numpy(), w_final, b_final)\n",
    "\n",
    "def calculate_metrics(y_true, y_pred, set_name):\n",
    "    tp = np.sum((y_true == 1) & (y_pred == 1))\n",
    "    tn = np.sum((y_true == 0) & (y_pred == 0))\n",
    "    fp = np.sum((y_true == 0) & (y_pred == 1))\n",
    "    fn = np.sum((y_true == 1) & (y_pred == 0))\n",
    "    \n",
    "    accuracy = (tp + tn) / len(y_true)\n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    \n",
    "    print(f\"--- {set_name} Metrics ---\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "    print()\n",
    "\n",
    "calculate_metrics(y_train, y_pred_train, \"Train\")\n",
    "calculate_metrics(y_test, y_pred_test, \"Test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Visualize Decision Boundaries\n",
    "Select â‰¥3 feature pairs, train 2D models, and plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_decision_boundary(w, b, X, y, feature_names):\n",
    "    # Plot data points\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.scatter(X[y==0, 0], X[y==0, 1], c='blue', label='Absence (0)', marker='o')\n",
    "    plt.scatter(X[y==1, 0], X[y==1, 1], c='red', label='Presence (1)', marker='x')\n",
    "    \n",
    "    # Calculate boundary\n",
    "    # w1*x1 + w2*x2 + b = 0 => x2 = -(w1*x1 + b) / w2\n",
    "    x1_min, x1_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5\n",
    "    x1_vals = np.linspace(x1_min, x1_max, 100)\n",
    "    \n",
    "    if w[1] != 0:\n",
    "        x2_vals = -(w[0] * x1_vals + b) / w[1]\n",
    "        plt.plot(x1_vals, x2_vals, 'k--', label='Decision Boundary')\n",
    "    else:\n",
    "        plt.axvline(x=-b/w[0], color='k', linestyle='--', label='Decision Boundary')\n",
    "        \n",
    "    plt.xlabel(feature_names[0])\n",
    "    plt.ylabel(feature_names[1])\n",
    "    plt.legend()\n",
    "    plt.title(f'Decision Boundary: {feature_names[0]} vs {feature_names[1]}')\n",
    "    plt.show()\n",
    "\n",
    "feature_pairs = [\n",
    "    ['Age', 'Cholesterol'],\n",
    "    ['BP', 'Max HR'],\n",
    "    ['ST depression', 'Number of vessels fluro']\n",
    "]\n",
    "\n",
    "for pair in feature_pairs:\n",
    "    # Subset data\n",
    "    X_pair_train = X_train[pair].to_numpy()\n",
    "    \n",
    "    # Train model on just these 2 features\n",
    "    w_pair_init = np.zeros(2)\n",
    "    w_pair, b_pair, _ = gradient_descent(X_pair_train, y_train.to_numpy(), w_pair_init, 0, 0.01, 1000)\n",
    "    \n",
    "    # Plot\n",
    "    plot_decision_boundary(w_pair, b_pair, X_pair_train, y_train.to_numpy(), pair)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Repeat with Regularization (L2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost_reg(X, y, w, b, lambda_reg):\n",
    "    m = len(y)\n",
    "    cost_no_reg = compute_cost(X, y, w, b)\n",
    "    reg_term = (lambda_reg / (2 * m)) * np.sum(w**2)\n",
    "    return cost_no_reg + reg_term\n",
    "\n",
    "def compute_gradient_reg(X, y, w, b, lambda_reg):\n",
    "    m = len(y)\n",
    "    dj_dw, dj_db = compute_gradient(X, y, w, b)\n",
    "    dj_dw += (lambda_reg / m) * w\n",
    "    return dj_dw, dj_db\n",
    "\n",
    "def gradient_descent_reg(X, y, w, b, alpha, num_iters, lambda_reg):\n",
    "    J_history = []\n",
    "    for i in range(num_iters):\n",
    "        dj_dw, dj_db = compute_gradient_reg(X, y, w, b, lambda_reg)\n",
    "        w = w - alpha * dj_dw\n",
    "        b = b - alpha * dj_db\n",
    "        \n",
    "        if i % 100 == 0:\n",
    "            J_history.append(compute_cost_reg(X, y, w, b, lambda_reg))\n",
    "            \n",
    "    return w, b, J_history\n",
    "\n",
    "lambdas = [0, 0.001, 0.01, 0.1, 1]\n",
    "\n",
    "print(\"--- Regularization Tuning ---\")\n",
    "for lam in lambdas:\n",
    "    w_reg, b_reg, _ = gradient_descent_reg(X_train.to_numpy(), y_train.to_numpy(), \n",
    "                                           np.zeros(X_train.shape[1]), 0, \n",
    "                                           0.01, 1000, lam)\n",
    "    \n",
    "    y_pred_tun = predict(X_test.to_numpy(), w_reg, b_reg)\n",
    "    acc = np.mean(y_pred_tun == y_test)\n",
    "    print(f\"Lambda: {lam}, Test Accuracy: {acc:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
